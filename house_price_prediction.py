# -*- coding: utf-8 -*-
"""house_price_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EKljfYX5lojwXfHG78rh-9kL6SjG2uz8

# PREDIKSI HARGA RUMAH DI CALIFORNIA - MACHINE LEARNING PROJECT
# Author: Muhammad Razi Al Kindi Nadra

# 1. IMPORT LIBRARY
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Set random seed untuk reproducibility
np.random.seed(42)

"""# 2. LOAD DATASET"""

print("Loading California Housing Dataset...")
# Load dataset dari sklearn
california_housing = fetch_california_housing()
X = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)
y = pd.Series(california_housing.target, name='MedHouseVal')

# Gabungkan features dan target untuk analisis
df = pd.concat([X, y], axis=1)

print(f"Dataset loaded successfully!")
print(f"Shape: {df.shape}")
print(f"Features: {list(X.columns)}")

"""# 3. DATA UNDERSTANDING"""

print("\n" + "="*50)
print("DATA UNDERSTANDING")
print("="*50)

# Informasi dataset
print("\nDataset Info:")
print(df.info())

# Statistik deskriptif
print("\nStatistical Summary:")
print(df.describe())

# Check missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Check duplicates
print(f"\nDuplicate rows: {df.duplicated().sum()}")

# Penjelasan fitur
print("\nFeature Descriptions:")
feature_descriptions = {
    'MedInc': 'Median income in block group',
    'HouseAge': 'Median house age in block group',
    'AveRooms': 'Average number of rooms per household',
    'AveBedrms': 'Average number of bedrooms per household',
    'Population': 'Block group population',
    'AveOccup': 'Average number of household members',
    'Latitude': 'Block group latitude',
    'Longitude': 'Block group longitude',
    'MedHouseVal': 'Median house value (target)'
}

for feature, description in feature_descriptions.items():
    print(f"- {feature}: {description}")

"""# 4. EXPLORATORY DATA ANALYSIS (EDA)"""

print("\n" + "="*50)
print("EXPLORATORY DATA ANALYSIS")
print("="*50)

# Visualisasi distribusi target
plt.figure(figsize=(15, 10))

plt.subplot(3, 3, 1)
plt.hist(df['MedHouseVal'], bins=50, edgecolor='black')
plt.title('Distribution of Median House Values')
plt.xlabel('Median House Value ($100,000s)')
plt.ylabel('Frequency')

# Visualisasi distribusi features
features_to_plot = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms',
                   'Population', 'AveOccup', 'Latitude', 'Longitude']

for i, feature in enumerate(features_to_plot, 2):
    plt.subplot(3, 3, i)
    plt.hist(df[feature], bins=30, edgecolor='black', alpha=0.7)
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
mask = np.triu(np.ones_like(correlation_matrix), k=1)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm',
            center=0, mask=mask, fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Scatter plots untuk features dengan korelasi tinggi
plt.figure(figsize=(15, 10))

# MedInc vs MedHouseVal
plt.subplot(2, 3, 1)
plt.scatter(df['MedInc'], df['MedHouseVal'], alpha=0.5)
plt.xlabel('Median Income')
plt.ylabel('Median House Value')
plt.title('House Value vs Income')

# HouseAge vs MedHouseVal
plt.subplot(2, 3, 2)
plt.scatter(df['HouseAge'], df['MedHouseVal'], alpha=0.5)
plt.xlabel('House Age')
plt.ylabel('Median House Value')
plt.title('House Value vs Age')

# AveRooms vs MedHouseVal
plt.subplot(2, 3, 3)
plt.scatter(df['AveRooms'], df['MedHouseVal'], alpha=0.5)
plt.xlabel('Average Rooms')
plt.ylabel('Median House Value')
plt.title('House Value vs Avg Rooms')

# Geographic distribution
plt.subplot(2, 3, 4)
scatter = plt.scatter(df['Longitude'], df['Latitude'],
                     c=df['MedHouseVal'], cmap='viridis', alpha=0.5)
plt.colorbar(scatter, label='Median House Value')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Geographic Distribution of House Values')

# Population density
plt.subplot(2, 3, 5)
plt.scatter(df['Population'], df['MedHouseVal'], alpha=0.5)
plt.xlabel('Population')
plt.ylabel('Median House Value')
plt.title('House Value vs Population')

# Average occupancy
plt.subplot(2, 3, 6)
plt.scatter(df['AveOccup'], df['MedHouseVal'], alpha=0.5)
plt.xlabel('Average Occupancy')
plt.ylabel('Median House Value')
plt.title('House Value vs Avg Occupancy')

plt.tight_layout()
plt.show()

# Analisis outliers menggunakan boxplot
plt.figure(figsize=(15, 5))
features_for_outliers = ['AveRooms', 'AveBedrms', 'Population', 'AveOccup']

for i, feature in enumerate(features_for_outliers, 1):
    plt.subplot(1, 4, i)
    df.boxplot(column=feature)
    plt.title(f'Boxplot of {feature}')
    plt.ylabel(feature)

plt.tight_layout()
plt.show()

"""# 5. DATA PREPARATION"""

print("\n" + "="*50)
print("DATA PREPARATION")
print("="*50)

# 5.1 Feature Engineering
print("\n5.1 Feature Engineering...")

# Create new features
df_prep = df.copy()

# Rooms per bedroom ratio
df_prep['RoomsPerBedroom'] = df_prep['AveRooms'] / (df_prep['AveBedrms'] + 0.001)

# Income per room
df_prep['IncomePerRoom'] = df_prep['MedInc'] / (df_prep['AveRooms'] + 0.001)

# Population density (population per household)
df_prep['PopulationDensity'] = df_prep['Population'] / (df_prep['AveOccup'] + 0.001)

# Geographic clusters based on lat/lon
df_prep['GeoCluster'] = pd.cut(df_prep['Latitude'], bins=5, labels=False) * 5 + \
                        pd.cut(df_prep['Longitude'], bins=5, labels=False)

# Age categories
df_prep['AgeCategory'] = pd.cut(df_prep['HouseAge'],
                                bins=[0, 10, 20, 30, 40, 100],
                                labels=['Very New', 'New', 'Medium', 'Old', 'Very Old'])

# Income categories
df_prep['IncomeCategory'] = pd.qcut(df_prep['MedInc'], q=5,
                                    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])

print("New features created successfully!")

# 5.2 Handling Outliers
print("\n5.2 Handling Outliers...")

# Remove extreme outliers using IQR method for specific features
features_to_check = ['AveRooms', 'AveBedrms', 'Population', 'AveOccup']

print(f"Original dataset size: {len(df_prep)}")

for feature in features_to_check:
    Q1 = df_prep[feature].quantile(0.25)
    Q3 = df_prep[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 3 * IQR  # Using 3*IQR for extreme outliers only
    upper_bound = Q3 + 3 * IQR

    outliers = ((df_prep[feature] < lower_bound) | (df_prep[feature] > upper_bound)).sum()
    print(f"Outliers in {feature}: {outliers}")

    df_prep = df_prep[(df_prep[feature] >= lower_bound) & (df_prep[feature] <= upper_bound)]

print(f"Dataset size after outlier removal: {len(df_prep)}")

# 5.3 Encoding Categorical Variables
print("\n5.3 Encoding Categorical Variables...")

# One-hot encoding for age categories
age_dummies = pd.get_dummies(df_prep['AgeCategory'], prefix='Age')
income_dummies = pd.get_dummies(df_prep['IncomeCategory'], prefix='Income')

# Add dummies to dataframe
df_prep = pd.concat([df_prep, age_dummies, income_dummies], axis=1)

# 5.4 Feature Selection
print("\n5.4 Feature Selection...")

# Select features for modeling
feature_cols = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population',
                'AveOccup', 'Latitude', 'Longitude', 'RoomsPerBedroom',
                'IncomePerRoom', 'PopulationDensity', 'GeoCluster']

# Add dummy variables
feature_cols.extend(age_dummies.columns.tolist())
feature_cols.extend(income_dummies.columns.tolist())

X = df_prep[feature_cols]
y = df_prep['MedHouseVal']

print(f"Final feature set: {len(feature_cols)} features")
print(f"Dataset shape: {X.shape}")

# 5.5 Train-Test Split
print("\n5.5 Train-Test Split...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")

# 5.6 Feature Scaling
print("\n5.6 Feature Scaling...")

# Use RobustScaler for features with outliers
scaler = RobustScaler()

# Scale only numerical features
numerical_features = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population',
                     'AveOccup', 'Latitude', 'Longitude', 'RoomsPerBedroom',
                     'IncomePerRoom', 'PopulationDensity']

X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])

print("Feature scaling completed!")

"""# 6. MODELING

"""

print("\n" + "="*50)
print("MODELING")
print("="*50)

# Dictionary to store models and results
models = {}
results = {}

# 6.1 Linear Regression (Baseline)
print("\n6.1 Training Linear Regression...")
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)
models['Linear Regression'] = lr_model

# 6.2 Ridge Regression
print("\n6.2 Training Ridge Regression...")
ridge_model = Ridge(alpha=1.0, random_state=42)
ridge_model.fit(X_train_scaled, y_train)
models['Ridge Regression'] = ridge_model

# 6.3 Decision Tree Regressor
print("\n6.3 Training Decision Tree...")
dt_model = DecisionTreeRegressor(max_depth=10, min_samples_split=20,
                                 min_samples_leaf=10, random_state=42)
dt_model.fit(X_train, y_train)  # No scaling needed for tree-based models
models['Decision Tree'] = dt_model

# 6.4 Random Forest with Hyperparameter Tuning
print("\n6.4 Training Random Forest with Hyperparameter Tuning...")

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Initialize Random Forest
rf = RandomForestRegressor(random_state=42, n_jobs=-1)

# Grid Search with Cross Validation
print("Performing Grid Search...")
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                          cv=5, n_jobs=-1, verbose=1,
                          scoring='neg_mean_squared_error')

grid_search.fit(X_train, y_train)
best_rf_model = grid_search.best_estimator_
models['Random Forest'] = best_rf_model

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {-grid_search.best_score_:.4f}")

# 6.5 Gradient Boosting Regressor
print("\n6.5 Training Gradient Boosting...")
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,
                                     max_depth=5, random_state=42)
gb_model.fit(X_train, y_train)
models['Gradient Boosting'] = gb_model

"""# 7. EVALUATION"""

print("\n" + "="*50)
print("MODEL EVALUATION")
print("="*50)

def evaluate_model(model, X_test, y_test, model_name):
    """Evaluate model performance"""
    # Make predictions
    if model_name in ['Linear Regression', 'Ridge Regression']:
        y_pred = model.predict(X_test_scaled)
    else:
        y_pred = model.predict(X_test)

    # Calculate metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    # Calculate MAPE (Mean Absolute Percentage Error)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

    # Store results
    results[model_name] = {
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'R2': r2,
        'MAPE': mape,
        'Predictions': y_pred
    }

    print(f"\n{model_name} Performance:")
    print(f"MAE:  {mae:.4f}")
    print(f"MSE:  {mse:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"R²:   {r2:.4f}")
    print(f"MAPE: {mape:.2f}%")

    return results[model_name]

# Evaluate all models
for model_name, model in models.items():
    evaluate_model(model, X_test, y_test, model_name)

# Model Comparison
print("\n" + "="*50)
print("MODEL COMPARISON")
print("="*50)

comparison_df = pd.DataFrame({
    model_name: {
        'MAE': results[model_name]['MAE'],
        'RMSE': results[model_name]['RMSE'],
        'R²': results[model_name]['R2'],
        'MAPE': results[model_name]['MAPE']
    }
    for model_name in results.keys()
}).T

print(comparison_df.round(4))

# Find best model
best_model_name = comparison_df['R²'].idxmax()
print(f"\nBest Model: {best_model_name} (R² = {comparison_df.loc[best_model_name, 'R²']:.4f})")

"""# 8. VISUALIZATION OF RESULTS"""

print("\n" + "="*50)
print("VISUALIZING RESULTS")
print("="*50)

# Predictions vs Actual plots
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for idx, (model_name, result) in enumerate(results.items()):
    ax = axes[idx]
    y_pred = result['Predictions']

    ax.scatter(y_test, y_pred, alpha=0.5, s=10)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
            'r--', lw=2)
    ax.set_xlabel('Actual Values')
    ax.set_ylabel('Predicted Values')
    ax.set_title(f'{model_name}\nR² = {result["R2"]:.4f}')
    ax.grid(True, alpha=0.3)

# Remove empty subplot
if len(results) < 6:
    fig.delaxes(axes[-1])

plt.tight_layout()
plt.show()

# Residual plots for best model
best_model = models[best_model_name]
if best_model_name in ['Linear Regression', 'Ridge Regression']:
    y_pred_best = best_model.predict(X_test_scaled)
else:
    y_pred_best = best_model.predict(X_test)

residuals = y_test - y_pred_best

plt.figure(figsize=(15, 5))

# Residual distribution
plt.subplot(1, 3, 1)
plt.hist(residuals, bins=50, edgecolor='black')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals')

# Residuals vs Predicted
plt.subplot(1, 3, 2)
plt.scatter(y_pred_best, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Predicted Values')

# Q-Q plot
plt.subplot(1, 3, 3)
from scipy import stats
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q Plot')

plt.tight_layout()
plt.show()

# Feature Importance (for tree-based models)
tree_models = ['Decision Tree', 'Random Forest', 'Gradient Boosting']
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for idx, model_name in enumerate(tree_models):
    if model_name in models:
        model = models[model_name]
        importances = model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'Feature': X_train.columns,
            'Importance': importances
        }).sort_values('Importance', ascending=False).head(15)

        ax = axes[idx]
        ax.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
        ax.set_xlabel('Importance')
        ax.set_title(f'Feature Importance - {model_name}')
        ax.invert_yaxis()

plt.tight_layout()
plt.show()

"""# 9. CROSS-VALIDATION

"""

print("\n" + "="*50)
print("CROSS-VALIDATION RESULTS")
print("="*50)

# Perform 5-fold cross-validation for best model
cv_scores = cross_val_score(best_model, X_train, y_train,
                           cv=5, scoring='neg_mean_squared_error')
cv_rmse_scores = np.sqrt(-cv_scores)

print(f"\n{best_model_name} Cross-Validation Results:")
print(f"CV RMSE scores: {cv_rmse_scores}")
print(f"Mean CV RMSE: {cv_rmse_scores.mean():.4f} (+/- {cv_rmse_scores.std() * 2:.4f})")

"""# 10. MODEL TESTING WITH SAMPLE DATA"""

print("\n" + "="*50)
print("TESTING WITH SAMPLE DATA")
print("="*50)

# Create sample data
sample_data = pd.DataFrame({
    'MedInc': [3.5, 5.0, 8.0],
    'HouseAge': [25, 10, 35],
    'AveRooms': [5.5, 6.2, 7.1],
    'AveBedrms': [1.1, 1.0, 1.2],
    'Population': [3000, 2500, 1800],
    'AveOccup': [3.0, 2.5, 2.2],
    'Latitude': [34.05, 33.95, 34.15],
    'Longitude': [-118.25, -118.35, -118.15]
})

print("Sample house characteristics:")
print(sample_data)

# Prepare sample data
sample_prep = sample_data.copy()

# Feature engineering
sample_prep['RoomsPerBedroom'] = sample_prep['AveRooms'] / (sample_prep['AveBedrms'] + 0.001)
sample_prep['IncomePerRoom'] = sample_prep['MedInc'] / (sample_prep['AveRooms'] + 0.001)
sample_prep['PopulationDensity'] = sample_prep['Population'] / (sample_prep['AveOccup'] + 0.001)
sample_prep['GeoCluster'] = 12  # Example cluster

# Add dummy variables (using most common categories)
for col in X_train.columns:
    if col not in sample_prep.columns:
        sample_prep[col] = 0

sample_prep = sample_prep[X_train.columns]

# Scale if needed
if best_model_name in ['Linear Regression', 'Ridge Regression']:
    sample_prep[numerical_features] = scaler.transform(sample_prep[numerical_features])

# Make predictions
predictions = best_model.predict(sample_prep)

print(f"\nPredicted house values using {best_model_name}:")
for i, pred in enumerate(predictions):
    print(f"House {i+1}: ${pred*100000:,.2f}")

"""# 11. SAVE MODEL"""

print("\n" + "="*50)
print("SAVING MODEL")
print("="*50)

import joblib

# Save best model and scaler
joblib.dump(best_model, 'california_house_price_model.pkl')
joblib.dump(scaler, 'california_house_price_scaler.pkl')
joblib.dump(feature_cols, 'california_house_features.pkl')

print(f"Best model ({best_model_name}) saved successfully!")
print("Scaler and feature list saved successfully!")

"""# 12. PROJECT SUMMARY"""

print("\n" + "="*50)
print("PROJECT SUMMARY")
print("="*50)

print(f"\nDataset Information:")
print(f"- Total samples: {len(df)}")
print(f"- Samples after cleaning: {len(df_prep)}")
print(f"- Number of features: {len(feature_cols)}")
print(f"- Target variable: Median House Value (in $100,000s)")

print(f"\nBest Model: {best_model_name}")
print(f"- R² Score: {results[best_model_name]['R2']:.4f}")
print(f"- RMSE: ${results[best_model_name]['RMSE']*100000:,.2f}")
print(f"- MAE: ${results[best_model_name]['MAE']*100000:,.2f}")
print(f"- MAPE: {results[best_model_name]['MAPE']:.2f}%")

print(f"\nKey Insights:")
print(f"- The model explains {results[best_model_name]['R2']*100:.1f}% of variance in house prices")
print(f"- Average prediction error is approximately ${results[best_model_name]['RMSE']*100000:,.0f}")
print(f"- Most important features include median income, location, and house characteristics")

# Check if we meet the target
target_rmse_threshold = y.mean() * 0.2  # 20% of mean price
if results[best_model_name]['RMSE'] < target_rmse_threshold:
    print(f"\n✅ SUCCESS: RMSE ({results[best_model_name]['RMSE']:.4f}) < 20% of mean price ({target_rmse_threshold:.4f})")
else:
    print(f"\n❌ Target not met: RMSE ({results[best_model_name]['RMSE']:.4f}) > 20% of mean price ({target_rmse_threshold:.4f})")

print("\n" + "="*50)
print("PROJECT COMPLETED SUCCESSFULLY!")
print("="*50)